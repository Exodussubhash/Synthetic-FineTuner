{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05439a33",
   "metadata": {},
   "source": [
    "\n",
    "# üß† Synthetic Fine‚ÄëTuner ‚Äî QLoRA on Mistral‚Äë7B\n",
    "\n",
    "This notebook demonstrates **LoRA/QLoRA** fine‚Äëtuning of `mistralai/Mistral-7B-Instruct-v0.3` on a custom Q&A dataset about Transformers.\n",
    "\n",
    "**Pipeline**\n",
    "1. Load & clean the dataset  \n",
    "2. Configure LoRA adapters (+ 4‚Äëbit quantization)  \n",
    "3. Fine‚Äëtune with Hugging Face `Trainer`  \n",
    "4. Monitor training loss live  \n",
    "5. Evaluate (BLEU + semantic similarity)  \n",
    "6. Quick inference sanity checks\n",
    "\n",
    "> Tip: Run `Kernel ‚Üí Restart & Run All` once dependencies are installed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b6934",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc03f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U transformers==4.57.1 peft==0.17.1 accelerate bitsandbytes\n",
    "# %pip install -U datasets pandas sentencepiece tokenizers sacrebleu sentence-transformers tqdm wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b6c7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ap24027/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-20 00:30:01.060423: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-20 00:30:01.126375: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-20 00:30:17.055965: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os, json, math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import re\n",
    "from typing import List, Dict\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1df4a0",
   "metadata": {},
   "source": [
    "## üìÅ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c44064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"         # or a local path if pre-downloaded\n",
    "DATA_PATH  = \"transformers_200Q.jsonl\"  # <-- update if different\n",
    "OUTPUT_DIR = \"outputs/mistral7b-transformers-lora\"\n",
    "WANDB_PROJECT = None   # e.g., \"transformers-qa-finetune\" to enable W&B\n",
    "\n",
    "# Training knobs\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 2\n",
    "GRAD_ACCUM = 16\n",
    "LR = 1e-4\n",
    "MAX_SEQ_LEN = 2048\n",
    "LOG_STEPS = 1\n",
    "EVAL_STEPS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d5eca-ea6b-42bf-968a-5d3ff2c6a7fc",
   "metadata": {},
   "source": [
    "## \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125f393-117c-4ab0-9786-ee77130fa576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extract full text from the PDF.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = []\n",
    "    for page in reader.pages:\n",
    "        txt = page.extract_text()\n",
    "        if txt:\n",
    "            text.append(txt)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def extract_qa_pairs(text: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract question-answer pairs of the form:\n",
    "    Question N:\\n<question>\\n<answer>\n",
    "    \"\"\"\n",
    "    # Normalize spacing\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Match patterns like \"Question 12: What is ...\" capturing Q & A\n",
    "    pattern = re.compile(r'(Question\\s*\\d+\\s*:\\s*)([^?]+?\\?)\\s*(.*?)(?=Question\\s*\\d+\\s*:|$)', re.IGNORECASE)\n",
    "    pairs = []\n",
    "    for i, match in enumerate(pattern.finditer(text), 1):\n",
    "        question = match.group(2).strip()\n",
    "        answer = match.group(3).strip()\n",
    "        if len(answer) > 15:  # simple filter to skip broken lines\n",
    "            pairs.append({\"id\": i, \"question\": question, \"answer\": answer})\n",
    "    return pairs\n",
    "\n",
    "def save_jsonl(pairs: List[Dict], out_path: str):\n",
    "    \"\"\"Write extracted pairs to JSONL.\"\"\"\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for p in pairs:\n",
    "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "pdf_path = \"transformers_200Qs_v11.pdf\"  #  file path\n",
    "out_path = \"./transformers_200Q.jsonl\"\n",
    "\n",
    "text = extract_text(pdf_path)\n",
    "qa_pairs = extract_qa_pairs(text)\n",
    "# save_jsonl(qa_pairs, out_path)\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(qa_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f48096b",
   "metadata": {},
   "source": [
    "## üßπ Load & Preview Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100e898f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What is a neural network in the context of art...</td>\n",
       "      <td>A neural network is a computational model insp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What is deep learning and how is it different ...</td>\n",
       "      <td>Deep learning is a subfield of machine learnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Why are neural networks considered universal f...</td>\n",
       "      <td>Neural networks are capable of approximating a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>What are activation functions and why are they...</td>\n",
       "      <td>Activation functions introduce non-linearity i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What is backpropagation and how does it enable...</td>\n",
       "      <td>Backpropagation is an algorithm used to train ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           question  \\\n",
       "0   1  What is a neural network in the context of art...   \n",
       "1   2  What is deep learning and how is it different ...   \n",
       "2   3  Why are neural networks considered universal f...   \n",
       "3   4  What are activation functions and why are they...   \n",
       "4   5  What is backpropagation and how does it enable...   \n",
       "\n",
       "                                              answer  \n",
       "0  A neural network is a computational model insp...  \n",
       "1  Deep learning is a subfield of machine learnin...  \n",
       "2  Neural networks are capable of approximating a...  \n",
       "3  Activation functions introduce non-linearity i...  \n",
       "4  Backpropagation is an algorithm used to train ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load JSONL with fields: id (opt), question, answer\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    rows = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "print(f\"Loaded {len(rows)} rows\")\n",
    "pd.DataFrame(rows[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4347f8",
   "metadata": {},
   "source": [
    "## üß© Tokenizer & Model (QLoRA config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341eef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:28<00:00,  9.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41,943,040 || all params: 7,289,966,592 || trainable%: 0.5754\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# 4-bit quantization via bitsandbytes (QLoRA base)\n",
    "bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA adapters\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05, task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da555abf",
   "metadata": {},
   "source": [
    "## üîß Data Collator & Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7866a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to HF datasets and split\n",
    "ds = Dataset.from_pandas(pd.DataFrame(rows)).train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Keep only needed columns for collator\n",
    "for split in [\"train\",\"test\"]:\n",
    "    keep = {\"question\",\"answer\"}\n",
    "    drop = [c for c in ds[split].column_names if c not in keep]\n",
    "    ds[split] = ds[split].remove_columns(drop)\n",
    "\n",
    "def collate(batch):\n",
    "    qs  = [b[\"question\"] for b in batch]\n",
    "    ans = [b[\"answer\"]   for b in batch]\n",
    "    texts = [f\"<s>[INST] {q} [/INST] {a}\" for q,a in zip(qs, ans)]\n",
    "    toks = tok(texts, truncation=True, padding=True, max_length=MAX_SEQ_LEN, return_tensors=\"pt\")\n",
    "    toks[\"labels\"] = toks[\"input_ids\"].clone()\n",
    "    toks[\"labels\"][toks[\"labels\"] == tok.pad_token_id] = -100\n",
    "    return toks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b663ac9e",
   "metadata": {},
   "source": [
    "## üîÅ Training (with live loss logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a4abd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/6494467.1.all.q/ipykernel_1149875/3913749127.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 01:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.730100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.890700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.274700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.313500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.297600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.950600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.879700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.626600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.728100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.749100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.527200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.536800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training complete ‚Üí outputs/mistral7b-transformers-lora\n",
      "Final train loss: 2.07542044321696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "report_to = [\"wandb\"] if WANDB_PROJECT else []\n",
    "if WANDB_PROJECT:\n",
    "    import wandb\n",
    "    wandb.init(project=WANDB_PROJECT, name=Path(OUTPUT_DIR).name, config={\n",
    "        \"epochs\": EPOCHS, \"batch_size\": BATCH_SIZE, \"grad_accum\": GRAD_ACCUM,\n",
    "        \"lr\": LR, \"max_seq_len\": MAX_SEQ_LEN\n",
    "    })\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    bf16=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=LOG_STEPS,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_steps=EVAL_STEPS,\n",
    "    save_total_limit=1,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=report_to,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    data_collator=collate,\n",
    "    tokenizer=tok,\n",
    ")\n",
    "\n",
    "train_output = trainer.train()\n",
    "print(\"‚úÖ Training complete ‚Üí\", OUTPUT_DIR)\n",
    "print(\"Final train loss:\", train_output.training_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65975daf",
   "metadata": {},
   "source": [
    "## üìâ Evaluation (Perplexity, BLEU, Semantic Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "808855d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 2.0016 | Perplexity: 7.401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:28<00:00,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ‚Üí outputs/mistral7b-transformers-lora/eval_results.csv\n",
      "BLEU mean: 7.763581669168518\n",
      "Cosine mean: 0.7954990267753601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>reference</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>bleu</th>\n",
       "      <th>cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is TensorFlow Lite?</td>\n",
       "      <td>TensorFlow Lite is a lightweight version of Te...</td>\n",
       "      <td>TensorFlow Lite is a framework for deploying m...</td>\n",
       "      <td>7.452870</td>\n",
       "      <td>0.877083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the encoder-decoder structure in trans...</td>\n",
       "      <td>The encoder-decoder structure consists of two ...</td>\n",
       "      <td>Encoder-decoder structures are used for tasks ...</td>\n",
       "      <td>7.314799</td>\n",
       "      <td>0.748851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is attention dropout?</td>\n",
       "      <td>Attention dropout is applied to the attention ...</td>\n",
       "      <td>Attention dropout randomly masks attention wei...</td>\n",
       "      <td>3.489212</td>\n",
       "      <td>0.780709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How are image patches encoded in Vision Transf...</td>\n",
       "      <td>In ViT, an image is divided into non-overlappi...</td>\n",
       "      <td>Vision Transformers (ViTs) encode image patche...</td>\n",
       "      <td>5.582553</td>\n",
       "      <td>0.705827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How are embeddings used in transformers?</td>\n",
       "      <td>Transformers use learned embeddings to represe...</td>\n",
       "      <td>Embeddings are used to represent input tokens ...</td>\n",
       "      <td>6.086276</td>\n",
       "      <td>0.727321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                           What is TensorFlow Lite?   \n",
       "1  What is the encoder-decoder structure in trans...   \n",
       "2                         What is attention dropout?   \n",
       "3  How are image patches encoded in Vision Transf...   \n",
       "4           How are embeddings used in transformers?   \n",
       "\n",
       "                                           reference  \\\n",
       "0  TensorFlow Lite is a lightweight version of Te...   \n",
       "1  The encoder-decoder structure consists of two ...   \n",
       "2  Attention dropout is applied to the attention ...   \n",
       "3  In ViT, an image is divided into non-overlappi...   \n",
       "4  Transformers use learned embeddings to represe...   \n",
       "\n",
       "                                          hypothesis      bleu    cosine  \n",
       "0  TensorFlow Lite is a framework for deploying m...  7.452870  0.877083  \n",
       "1  Encoder-decoder structures are used for tasks ...  7.314799  0.748851  \n",
       "2  Attention dropout randomly masks attention wei...  3.489212  0.780709  \n",
       "3  Vision Transformers (ViTs) encode image patche...  5.582553  0.705827  \n",
       "4  Embeddings are used to represent input tokens ...  6.086276  0.727321  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Optional extra metrics\n",
    "# %pip install -U sacrebleu sentence-transformers tqdm\n",
    "\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "def generate_answer(question: str, max_new_tokens: int = 128) -> str:\n",
    "    # Safer: use chat template to avoid prompt echo\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    input_ids = tok.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    input_len = input_ids.shape[-1]\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False, temperature=0.0,\n",
    "            eos_token_id=tok.eos_token_id, pad_token_id=tok.eos_token_id,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "    gen_ids = out.sequences[:, input_len:]\n",
    "    return tok.decode(gen_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "# Quick perplexity on eval split\n",
    "import math\n",
    "eval_metrics = trainer.evaluate()\n",
    "ppl = math.exp(eval_metrics[\"eval_loss\"])\n",
    "print(f\"Eval loss: {eval_metrics['eval_loss']:.4f} | Perplexity: {ppl:.3f}\")\n",
    "\n",
    "# BLEU + cosine on a capped subset for speed\n",
    "CAP = 10  # e.g., 100\n",
    "subset = ds[\"test\"].select(range(min(len(ds[\"test\"]), CAP))) if CAP else ds[\"test\"]\n",
    "\n",
    "sbert = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "records = []\n",
    "for ex in tqdm(subset, desc=\"Evaluating\"):\n",
    "    q, ref = ex[\"question\"].strip(), ex[\"answer\"].strip()\n",
    "    hyp = generate_answer(q, max_new_tokens=96)\n",
    "    bleu = sacrebleu.sentence_bleu(hyp, [ref]).score\n",
    "    emb = sbert.encode([hyp, ref], convert_to_tensor=True, normalize_embeddings=True)\n",
    "    cos = float(util.cos_sim(emb[0], emb[1]).item())\n",
    "    records.append({\"question\": q, \"reference\": ref, \"hypothesis\": hyp, \"bleu\": bleu, \"cosine\": cos})\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(Path(OUTPUT_DIR) / \"eval_results.csv\", index=False)\n",
    "print(\"Saved ‚Üí\", Path(OUTPUT_DIR) / \"eval_results.csv\")\n",
    "print(\"BLEU mean:\", df[\"bleu\"].mean() if not df.empty else \"n/a\")\n",
    "print(\"Cosine mean:\", df[\"cosine\"].mean() if not df.empty else \"n/a\")\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ab64c",
   "metadata": {},
   "source": [
    "## üí¨ Example Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f6638ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The exploding gradient problem occurs when gradients grow exponentially during backpropagation, causing instability in training. This is often seen in transformer models with large vocabularies or deep architectures. To mitigate this, techniques like gradient clipping or layer normalization\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"What is the exploding gradient problem?\"\n",
    "print(generate_answer(question, max_new_tokens=56))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e44c8bf",
   "metadata": {},
   "source": [
    "\n",
    "## üìä Results Summary (Fill after running)\n",
    "- **Training loss (final): *2.07* ``  \n",
    "- **Eval loss: *2.0016* ``  \n",
    "- **Perplexity:** `7.401`  \n",
    "- **BLEU mean:** `7.763`  \n",
    "- **Cosine mean:** `0.795`  \n",
    "\n",
    "**Key Takeaways**\n",
    "- QLoRA enables efficient 7B fine‚Äëtuning on modest GPUs via 4‚Äëbit quantization + LoRA adapters\n",
    "- Trainer logs training loss every few steps; `eval_results.csv` contains per‚Äësample metrics\n",
    "- Use the chat template for clean decoding without prompt echo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef39ed3c",
   "metadata": {},
   "source": [
    "## üíæ Save Adapters & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca4214a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA adapters & tokenizer ‚Üí outputs/mistral7b-transformers-lora\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tok.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved LoRA adapters & tokenizer ‚Üí\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785fa791-eef7-412d-8fdf-b9e55d8ee3db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
