# ğŸ§  Synthetic Fine-Tuner (QLoRA on Mistral-7B)

This mini-project demonstrates LoRA/QLoRA fine-tuning of **Mistral-7B-Instruct** on a Q&A dataset about *Transfer Learning & Transformers*.
Itâ€™s built as a portfolio project showing real-world LLM fine-tuning, evaluation, and efficient GPU usage.

---

### ğŸ” Overview
- **Model**: mistralai/Mistral-7B-Instruct-v0.3  
- **Technique**: QLoRA (4-bit quantization + LoRA adapters)  
- **Dataset**: 200 questions about Transformers (from `transformers_200Qs_v11.pdf`)
- **Metrics**: Training loss, Eval loss, BLEU, Semantic Similarity (SBERT)
- **Tools**: Hugging Face Transformers, PEFT, bitsandbytes, Accelerate, W&B

---

### ğŸ§© Pipeline
1. **Data Extraction** â†’ parse PDF into clean JSONL 
2. **QLoRA Fine-tuning**
3. **Evaluation** â†’ (BLEU + cosine similarity)
4. **Results**
   - Train loss â†“ from 2.58 â†’ 0.29  
   - Eval loss â‰ˆ 2.08 â†’ Perplexity â‰ˆ 8.0  
   - Semantic similarity â‰ˆ 0.86 average

---
